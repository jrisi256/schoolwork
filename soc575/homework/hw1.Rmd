---
title: 'Homework #1 - Joe Risi'
output: pdf_document
---

Some takeaways from the lecture and feedback on the homework:

* Remember categorical variables can be used to assess non-linearity as well as introducing interaction terms.
* You can use F-tests to assess if a model has better fit when the models are nested. You can use BIC when the the models aren't nested (like when you have dummy variables). This can help determine how likely it is there is non-linearity in the model and what form it takes.
* When we have small bucket sizes, don't be afraid to combine categories. You just have to do it in a way which makes sense. You want to combine like categories as much as possible.

```{r, echo = F}
knitr::opts_chunk$set(echo = F)
```

```{r, message = F, warning = F}
library(mlr)
library(here)
library(dplyr)
library(purrr)
library(tidyr)
library(haven)
library(broom)
library(ggplot2)
library(forcats)
library(stringr)
library(reghelper)
library(sjlabelled)
```

```{r}
data <- haven::read_dta(here("class data.dta"))

# Filter out missings, recode family income, make dummy variables
dataClean <-
    data %>%
    select(bys45, byfaminc, sex, bygrads) %>%
    filter(across(everything(), ~!is.na(.x)))

dataWide <-
    dataClean %>%
    mutate(byfaminc = as_character(byfaminc),
           byfaminc = fct_recode(byfaminc,
                                 "Less than $10,000" = "none",
                                 "Less than $10,000" = "less than $1,000",
                                 "Less than $10,000" = "$1,000 - $2,999",
                                 "Less than $10,000" = "$3,000 - $4,999",
                                 "Less than $10,000"= "$5,000 - $7,499",
                                 "Less than $10,000" = "$7,500 - $9,999",
                                 "$10,000 - $19,999" = "$10,000-$14,999",
                                 "$10,000 - $19,999" = "$15,000-$19,999",
                                 "$75,000 and above" = "$75,000-$99,999",
                                 "$75,000 and above" = "$100,000-199,999",
                                 "$75,000 and above" = "$200,000 or more"),
           byfaminc = fct_relevel(byfaminc,
                                  "Less than $10,000",
                                  "$10,000 - $19,999",
                                  "$20,000-$24,999",
                                  "$25,000-$34,999",
                                  "$35,000-$49,999",
                                  "$50,000-$74,999",
                                  "$75,000 and above"),
           bys45 = as.factor(as_character(bys45)),
           sex = as.factor(as_character(sex)),
           bygrads = as.numeric(bygrads)) %>%
        createDummyFeatures()

colnames(dataWide) <-
    str_replace(colnames(dataWide), "bys45.|byfaminc\\.{1,2}|sex.", "")

# Rearrange columns and drop reference categories
dataWide <-
    dataWide %>%
    relocate("won.t.finish.h.s", "will.finish.h.s", "voc.trd.bus.aftr.h.s",
             "will.attend.college", "will.finish.college") %>%
    select(-female, -higher.sch.aftr.coll, -`75.000.and.above`)
```

## Question 1

1. I filter out all missing observations. I go from `r nrow(data)` records to `r nrow(dataClean)` records.
2. Recode family income into roughly equal sizes. The new categories are as follows:
    * 0 - 9,999
    * 10,000 - 19,999
    * 20,000 - 24,999
    * 25,000 - 34,999
    * 35,000 - 49,999
    * 50,000 - 74,999
    * 75,000 and above
3. I turn sex, byfaminc, and bys45 into dummy variables.
4. I drop the following categories which will serve as the reference categories for the regression:
    * female (sex)
    * 75,000 and above (byfaminc)
    * higher.sch.aftr.coll (bys45)

```{r}
# Run regressions
linearModel <- lm(bygrads ~ ., data = dataWide)
summary(linearModel)
```

* All results are significant at the 0.05 level. All except one ($50,000 - %74,999) are significant at the 0.001 level.
* Relative to females while holding all other variables in the model constant, being male decreases one's GPA by about 0.1 points, on average.
* Relative to those students who come from families making more $75,000 or more each year while holding all other variables in the model constant:
    * Coming from a family making less than $10,000 decreases your GPA by 0.34418 points on average.
    * Coming from a family making between \$10,000 - \$19,999 decreases your GPA by 0.22947 points on average.
    * Coming from a family making between \$20,000 - \$24,999 decreases your GPA by 0.16325 points on average.
    * Coming from a family making between \$25,000 - \$34,999 decreases your GPA by 0.15089 points on average.
    * Coming from a family making between \$35,000 - \$49,999 decreases your GPA by 0.10332 points on average.
    * Coming from a family making between \$50,000 - \$74,999 decreases your GPA by 0.06352 points on average.
* Relative to those students who have expectations of going beyond their college education while holding all other variables in the model constant:
    * Having expectations of not finishing high school decreases your GPA by 1.17818 points on average.
    * Having expectations of just finishing high school decreases your GPA by 0.92871 points on average.
    * Having expectations of going to vocational/trade school decreases your GPA by 0.65136 points on average.
    * Having expectations of attending college decreases your GPA by 0.60532 points on average.
    * Having expectations of finishing college decreases your GPA by 0.24041 points on average.

```{r}
beta(linearModel, x = F)
```

The above results represent **y-standardized** coefficients. It does not make sense to standardize my independent variables because they are all dummy variables. It's hard to interpret what a standard deviation in a dummy variable would mean.

* All results are significant at the 0.05 level. All except one ($50,000 - %74,999) are significant at the 0.001 level.
* Relative to females while holding all other variables in the model constant, being male decreases one's GPA by about 0.13942 standard deviations on average.
* Relative to those students who come from families making more $75,000 or more each year while holding all other variables in the model constant:
    * Coming from a family making less than $10,000 decreases your GPA by 0.47093 standard deviations.
    * Coming from a family making between \$10,000 - \$19,999 decreases your GPA by 0.31397 standard deviations on average.
    * Coming from a family making between \$20,000 - \$24,999 decreases your GPA by 0.22336 standard deviations on average.
    * Coming from a family making between \$25,000 - \$34,999 decreases your GPA by 0.20646 standard deviations on average.
    * Coming from a family making between \$35,000 - \$49,999 decreases your GPA by 0.14137 standard deviations on average.
    * Coming from a family making between \$50,000 - \$74,999 decreases your GPA by 0.08691 standard deviations on average.
* Relative to those students who have expectations of going beyond their college education while holding all other variables in the model constant:
    * Having expectations of not finishing high school decreases your GPA by 1.61205 standard deviations on average.
    * Having expectations of just finishing high school decreases your GPA by 1.27070 standard deviations on average.
    * Having expectations of going to vocational/trade school decreases your GPA by 0.89122 standard deviations on average.
    * Having expectations of attending college decreases your GPA by 0.82823 standard deviations on average.
    * Having expectations of finishing college decreases your GPA by 0.32894 standard deviations on average.
    
```{r}
predict <- tibble(won.t.finish.h.s = c(1, 0, 0, 0, 0),
                  will.finish.h.s = c(0, 1, 0, 0, 0),
                  voc.trd.bus.aftr.h.s = c(0, 0, 1, 0, 0),
                  will.attend.college = c(0, 0, 0, 1, 0),
                  will.finish.college = c(0, 0, 0, 0, 1),
                  Less.than..10.000 = mean(dataWide$Less.than..10.000),
                  `10.000....19.999` = mean(dataWide$`10.000....19.999`),
                  `20.000..24.999` = mean(dataWide$`20.000..24.999`),
                  `25.000..34.999` = mean(dataWide$`25.000..34.999`),
                  `35.000..49.999` = mean(dataWide$`35.000..49.999`),
                  `50.000..74.999` = mean(dataWide$`50.000..74.999`),
                  `male` = mean(dataWide$male))

predictions <-
    as_tibble(predict(linearModel,
                      newdata = predict,
                      se.fit = T,
                      interval = "confidence",
                      level = 0.95)$fit) %>%
    bind_cols(predict) %>%
    pivot_longer(cols = matches("will|college|won|voc"),
                 names_to = "educational_expectations",
                 values_to = "values") %>%
    filter(values == 1) %>%
    select(-values) %>%
    mutate(educational_expectations = fct_relevel(educational_expectations,
                                                  "won.t.finish.h.s",
                                                  "will.finish.h.s",
                                                  "voc.trd.bus.aftr.h.s",
                                                  "will.attend.college",
                                                  "will.finish.college" ))

ggplot(predictions, aes(x = educational_expectations, y = fit)) +
    geom_point() +
    geom_errorbar(aes(ymin = lwr, ymax = upr)) +
    geom_line(group = 1) + 
    theme_bw() +
    labs(x = "Educational Expectations",
         y = "Expected GPA",
         title = "Expected value of GPA, holding family income and gender constant")
```

By holding family income and sex constant, I held each categorical variable at its mean value.

## Question 2

```{r}
dataClean2 <-
    data %>% 
    select(f4hhdg, f4hi99, f4gmrs) %>%
    filter(across(everything(), ~!is.na(.x))) %>%
    filter(f4hi99 != 0) %>%
    mutate(f4hhdg_squared = f4hhdg ^ 2)
```

1. I filter out all missing observations and all observations where the individual made $0. I go from `r nrow(data)` records to `r nrow(dataClean2)` records.
    * Another possible data cleaning decision could have been to drop all observations where f4aempl == 0 (the individual was not employed for pay). It seemed important to me to capture the fact that some of these individuals still reported income (there are some individual who reportedly didn't work but still reported income), and I do not drop them.
    * It is sometimes the case that incomes 5 times above the median or greater are dropped. I keep them and see if any of them show up as outliers later.
    * It can be also be common to practice to log transform the income variable. This is done to account for the right-hand skew of the data. However the data looks pretty normal (very few outlier values, see plot below) so I did not log transform the income variable.
2. I turn partnership status (f4gmrs) into a series of dummy variables, and I drop the single, never married category so it can serve as the reference.
    
```{r}
ggplot(dataClean2, aes(x = f4hi99)) + geom_histogram(bins = 100) + theme_bw()
```

```{r}
linearModel2 <- lm(f4hi99 ~ f4hhdg, data = dataClean2)
linearModel2_squared <- lm(f4hi99 ~ f4hhdg + f4hhdg_squared, data = dataClean2)
summary(linearModel2)
summary(linearModel2_squared)
anova(linearModel2, linearModel2_squared)
```

There is marginal evidence to suggest the effect of educational attainment on yearly income may be non-linear. A plot may prove useful.

```{r}
predict2 <- tibble(f4hhdg = as.numeric(sort(unique(dataClean2$f4hhdg))),
                    f4hhdg_squared = f4hhdg ^ 2)

predictions2 <-
    as_tibble(predict(linearModel2_squared,
                      newdata = predict2,
                      se.fit = T,
                      interval = "confidence",
                      level = 0.95)$fit) %>%
    bind_cols(predict2)

ggplot(predictions2, aes(x = f4hhdg, y = fit)) +
    geom_point() +
    geom_errorbar(aes(ymin = lwr, ymax = upr)) +
    geom_smooth(method = "lm", se = F, formula = y ~ x + I(x^2)) +
    theme_bw()
```

The graphical evidence suggests there may be a slight curve in the data with increasing returns to education (each extra unit of education produces a bigger bump in income than the previous unit increase). The evidence though is suggestive and not definitive.

```{r}
influence <-
    broom::augment(linearModel2) %>%
    mutate(id = row_number(),
           stdResidualSq = .std.resid ^ 2)

ggplot(influence, aes(x = stdResidualSq, y = .hat)) +
    geom_point() +
    geom_line(aes(x = .cooksd, y = .hat), color = "red") +
    theme_bw() +
    geom_text(aes(label = id))

noOutliers <-
    influence %>%
    filter(!(id %in% c(471, 7230, 6640, 3218, 2435)))

linearModelNoOutliers <- lm(f4hi99 ~ f4hhdg, data = noOutliers)
summary(linearModel2)
summary(linearModelNoOutliers)
```

There are some potential outliers but removing them from the linear regression doesn't change the model very much so the outliers aren't concerning.

```{r}
dataClean2a <-
    dataClean2 %>%
    mutate(f4hhdg_squared = f4hhdg ^ 2,
           f4hhdg = as.numeric(f4hhdg),
           f4hi99 = as.numeric(f4hi99),
           f4gmrs = as.factor(as_character(f4gmrs))) %>%
    createDummyFeatures() %>%
    select(-f4gmrs.single..never.married)

colnames(dataClean2a) <- str_replace(colnames(dataClean2a), "f4gmrs.", "")

linearModel2a <- lm(f4hi99 ~ ., data = dataClean2a)
summary(linearModel2a)
anova(linearModel2_squared, linearModel2a)
```

The ANOVA test indicates a statistically significant amount of more variation is explained when including the dummy variables for partnership status. However only a few of the partnership dummy variables are statistically significant, and the differences in the $R^2$ values between the two models is very small. Substantively these variables don't add much to the model.

* The coefficient for f4hhdg is a bit contextual as we're now stating the effect of education on income is dependent upon what specific level of education one has. It shall be interpreted below.
* The coefficient for the f4hhdg_squared term is positive suggesting the curve is convex meaning the more education one gets, the higher the boost in earnings one gets for each extra level of education obtained (statistically significant at the 0.1 level).
* If we wanted to get a sense for how much getting an Associate's Degree adds to your income, we can take the derivative:
    * Constant Effect of Education (fhddg) ~ 504.4
    * Changing Effect of Education (fhddg_squared) ~ 281.0 (5 for Associate's Degree, 2 for derivative term)
    * Boost in Income ~ 504.4 + 5 * 2 * 281 = 3314.4
    * Compare to the boost in income you get from having a Bachelor's Degree: 504.4 + 6 * 2 * 281 ~ 3876.4. Notice how the boost went up, consistent with our observation of the increasing returns of education.

* Relative to those who were single and never married as of 2000 while holding all other variables in the model constant:
    * Being divorced decreases your income by \$1285.6 on average.
    * Being in a marriage-like relationship increases your income by \$4097.0 on average (statistically significant at the 0.1 level).
    * Being in a marriage increases your income by \$993.1 on average (statistically significant at the 0.05 level).
    * Being separated decreases your income by \$3453.0 on average.
    * Being widowed decreases your income by \$21303.2 on average (only 1 person in the sample was widowed).
    
* What is the expected yearly income of a respondent who is married with an Associate's Degree?
    * Intercept ~ 23170.40
    * Married ~ 993.10
    * Constant Effect of Education (fhddg) ~ 504.4
    * Changing Effect of Education (fhddg_squared) ~ 281.0
    * 23170.40 + 993.10 + 5 * 504.4 + 281 * 25 ~ 33710.46

## Question 3

```{r}
dataClean3 <-
    data %>%
    select(f4gnch, f4gmrs, sex, f4aempl) %>%
    filter(across(everything(), ~!is.na(.x)))
```

1. I filter out all missing observations. I go from `r nrow(data)` records to `r nrow(dataClean3)` records.
2. I turn partnership status (f4gmrs) and sex (sex) into dummy variables. I drop males and those who were single, never married to serve as the reference groups.

```{r}
dataWide3 <-
    dataClean3 %>%
    mutate(f4gnch = as.numeric(f4gnch),
           f4gmrs = as.factor(as_character(f4gmrs)),
           sex = as.factor(as_character(sex)),
           f4aempl = as.numeric(f4aempl)) %>%
        createDummyFeatures() %>%
    select(-sex.male, -f4gmrs.single..never.married)

logitModel <- glm(f4aempl ~ ., data = dataWide3, family = "binomial")
summary(logitModel)

# Odds-Ratios
oddsRatios <- 100 * (exp(coef(logitModel)) - 1)
oddsRatios <- oddsRatios[names(oddsRatios) != "(Intercept)"]
oddsRatios
```

* The number of children is highly statistically significant as is being female. The relationship variables aren't really statistically significant except for being divorced (significant at the 0.05 level).
* Holding all other variables in the model constant, each additional child decreases the odds of being employed by 35.6% on average.
* Holding all other variables in the model constant, being female decreases the odds of being employed by 50.1% on average relative to males.
* Relative to people who are single and have never been married while holding all other variables in the model constant:
    * Being divorced increases the odds of being employed by 38.5% on average.
    * Being in a marriage-like relationship decreases the odds of being employed by 15.4% on average.
    * Being married increases the odds of being employed by 6.6% on average.
    * Being separated increases the odds of being employed by 14.0% on average.
    * Being widowed increases the odds of being employed by 40.1% on average.
    
```{r}
predict3 <- dataWide3 %>% select(-f4aempl)
predictions3 <- as_tibble(predict(logitModel,
                                  newdata = predict3,
                                  type = "response"))
ggplot(predictions3, aes(x = value)) + geom_histogram(bins = 40) + theme_bw()
```

Above are the predicted probabilities. As you can see, most people are predicted as being employed.

```{r}
predictAverageDf <-
    tibble(f4gnch = mean(dataWide3$f4gnch),
           f4gmrs.divorced = mean(dataWide3$f4gmrs.divorced),
           f4gmrs.in.marriage.like.relationship = mean(dataWide3$f4gmrs.in.marriage.like.relationship),
           f4gmrs.married = mean(dataWide3$f4gmrs.married),
           f4gmrs.separated = mean(dataWide3$f4gmrs.separated),
           f4gmrs.widowed = mean(dataWide3$f4gmrs.widowed),
           sex.female = mean(dataWide3$sex.female))

predictionAverage <- as_tibble(predict(logitModel,
                                       newdata = predictAverageDf,
                                       type = "response",
                                       se.fit = T))

predictMother3Df <-
    tibble(f4gnch = 3,
           f4gmrs.divorced = 0,
           f4gmrs.in.marriage.like.relationship = 0,
           f4gmrs.married = 0,
           f4gmrs.separated = 0,
           f4gmrs.widowed = 0,
           sex.female = 1)

predictionMother3 <- as_tibble(predict(logitModel,
                                       newdata = predictMother3Df,
                                       type = "response",
                                       se.fit = T))

predictFatherDf <-
    tibble(f4gnch = mean(dataWide3$f4gnch),
           f4gmrs.divorced = 0,
           f4gmrs.in.marriage.like.relationship = 0,
           f4gmrs.married = 1,
           f4gmrs.separated = 0,
           f4gmrs.widowed = 0,
           sex.female = 0)

predictionFather <- as_tibble(predict(logitModel,
                                       newdata = predictFatherDf,
                                       type = "response",
                                       se.fit = T))
```


* The predicted probability of being employed for the average respondent is ~0.8778169.
* The predicted probability of being employed for a single mother with exactly 3 children is ~0.6349911.
* The predicted probability of being employed as a married father is ~ 0.9140363.
