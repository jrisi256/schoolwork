---
title: "Weeks 4 and 5"
output: html_document
---

```{r, warning = F, message = F}
library(here)
library(dplyr)
library(purrr)
library(haven)
library(ggplot2)
library(forcats)
library(reghelper)
library(sjlabelled)
```

```{r}
data <- haven::read_dta(here("class data.dta"))
noMissing <-
    data %>%
    select(f2evdost, bypared, by2xrirr, bys43) %>%
    filter(across(everything(), ~!is.na(.x)))
```

There's a lot you can do with the smoking variable. You can treat them as dummy variables. You can re-code it to be binary. It may not make the most sense to be treating it as a continuous, numeric variable.

```{r}
model <- lm(f2evdost ~ bypared + by2xrirr + bys43, data = noMissing)
summary(model)

newData <-
    tibble(bys43 = unique(noMissing$bys43),
           bypared = rep(mean(noMissing$bypared), length(unique(noMissing$bys43))),
           by2xrirr = rep(mean(noMissing$by2xrirr), length(unique(noMissing$bys43))))

predict <-
    predict(model,
            newdata = newData,
            se.fit = T,
            interval = "confidence",
            level = 0.95)

graphPredictions <-
    newData %>%
    bind_cols(as_tibble(predict$fit))

ggplot(noMissing, aes(x = bys43, y = f2evdost)) +
    geom_point(data = graphPredictions, aes(x = bys43, y = fit)) +
    geom_errorbar(data = graphPredictions,
                  aes(x = bys43, ymin = lwr, ymax = upr),
                  inherit.aes = F) +
    #geom_line(aes(x = bys43, y = fit), data = graphPredictions, inherit.aes = F) + 
    geom_smooth(data = graphPredictions,
                aes(x = bys43, y = fit),
                method = "lm",
                se = F,
                formula = y ~ x,
                inherit.aes = F) +
    scale_x_continuous(label = c("i don^t smoke",
                               "1-5 cigarettes",
                               "about 1/2 pack",
                               "mt 1/2,lt 2 packs",
                               "2 packs or more"),
                       breaks = c(0, 1, 2, 3, 4)) +
    theme_bw()
```

## Logistic Regression

```{r}
logitModel <- glm(f2evdost ~ by2xrirr + bypared + bygrads + byfaminc + bys55f +
                      bys45 + bys53,
                  data = data,
                  family = "binomial")
summary(logitModel)

# Odds Ratios, for a unit increase in reading test scores, the odds of dropout
# are expected to decrease by a factor of 0.975 holding all other variables
# constant.

# For a unit increase in reading test scores, the odds of dropout decrease by
# 2.5% holding all other variables constant.
exp(coef(logitModel))
paste0(100 * (exp(coef(logitModel)) - 1), "%")

# Standardize the independent variables
# For a std. dev. increase in reading test scores, the odds of dropout
# decrease by about 20%.
exp(coef(beta(logitModel, y = F)))

noMissingLogit <-
    data %>%
    select(f2evdost, by2xrirr, bypared, bygrads, byfaminc, bys55f, bys45, bys53) %>%
    filter(across(everything(), ~!is.na(.x)))

# standard deviations
map(noMissingLogit, sd)
```

## Predicted Probabilities

```{r}
# Automatically (sort of)
predictDf <- noMissingLogit %>% select(-f2evdost)
predictions <- as_tibble(predict(logitModel,
                                 newdata = predictDf,
                                 type = "response"))
ggplot(predictions, aes(x = value)) + geom_histogram(bins = 35) + theme_bw()
```

For the average student you would do:
* Add expected natural log = intercept + logit coef_1 * mean_1 + logit_coef_2 * mean_2 + ...
* Expected probability = exp(expected natural log) / (1 + exp(expected natural log))
* Average student has "expected probability" chance of dropping out.
* For a series of dummy variables, if you want to find the predicted probability of one of those variables while holding everything else constant, you would set that dummy variable of interest to 1 while the others would be set to 0 NOT the mean.

```{r}
predictAverageDf <-
    tibble(by2xrirr = mean(noMissingLogit$by2xrirr),
           bypared = mean(noMissingLogit$bypared),
           bygrads = mean(noMissingLogit$bygrads),
           byfaminc = mean(noMissingLogit$byfaminc),
           bys55f = mean(noMissingLogit$bys55f),
           bys45 = mean(noMissingLogit$bys45),
           bys53 = mean(noMissingLogit$bys53))

predictionAverage <- as_tibble(predict(logitModel,
                                       newdata = predictAverageDf,
                                       type = "response",
                                       se.fit = T))
```

## In class exercise on voting

```{r}
# Missing
dataNoMissingVoting <-
    data %>%
    select(f4ivpre, f4hhdg, f4aempl, f2s30bc) %>%
    filter(across(everything(), ~!is.na(.x))) %>%
    filter(f2s30bc != 1)

# Logit
logitModelVoting <- glm(f4ivpre ~ f4hhdg + f4aempl + f2s30bc,
                        data = dataNoMissingVoting,
                        family = "binomial")
summary(logitModelVoting)

# Linear
linearModelVoting <- lm(f4ivpre ~ f4hhdg + f4aempl + f2s30bc,
                        data = dataNoMissingVoting)
summary(linearModelVoting)

# Predictions
predictDf <- dataNoMissingVoting %>% select(-f4ivpre)
predictionsLogit <- as_tibble(predict(logitModelVoting,
                                      newdata = predictDf,
                                      type = "response"))
predictionsOls <- as_tibble(predict(linearModelVoting,
                                    newdata = predictDf))

ggplot(predictionsLogit, aes(x = value)) +
    geom_histogram(bins = 35) +
    theme_bw()

ggplot(predictionsOls, aes(x = value)) +
    geom_histogram(bins = 35) +
    theme_bw()
```
