---
title: "Chp. 16 - Kmeans & Chp. 17 - Hierarchical"
output: html_document
---

```{r}
library(fpc)
library(mlr)
library(clue)
library(Hmisc)
library(mclust)
library(GGally)
library(clValid)
library(tidyverse)
library(clusterSim)
```

```{r}
data(GvHD, package = "mclust")
gvhdTib <- as_tibble(GvHD.control)
gvhdScaled <- gvhdTib %>% scale()
```

We use the upper, lower, and diag arguments to specify what kind of plots we want to be drawn above, below, and on the diagonal. Each argument takes a list where each list specifies a type of plot. 2D Density plots above the diagonal, scatterplots below, and density plots on the diagonal. You can wrap the plot type and include graphical options within the wrap.

```{r}
ggpairs(GvHD.control,
        upper = list(continuous = "density"),
        lower = list(continuous = wrap("points", size = 0.5)),
        diag = list(continuous = "densityDiag")) +
    theme_bw()
```

* The *par.vals* argument is for passing hyperparameters.
* *iter.max* sets an upper limit for the number of times the algorithm will cycle through the data.
* *nstart* is how many times the algorithm will randomly initialize the centers. For each set of initial centers, the cases are assigned to whatever cluster they're closest to like normal. The set with the smallest within-cluster sum of squared error is then the set used for the rest of the algorithm. Arguably more important than *iter.max* as it creates centroids more similar to the real centroids.

```{r}
gvhdTask <- makeClusterTask(data = as.data.frame(gvhdScaled))
kMeans <- makeLearner("cluster.kmeans", par.vals = list(iter.max = 100,
                                                        nstart = 10))
```

Choosing the number of clusters is hard. If you have some theoretical knowledge about what the number should be, then use that. If you're using it in a preprocessing step then you can include it as a hyperparameter in the model-building process, and pick whatever improves model performance the most.

People have proposed many different methods for measuring cluster over and underfitting. They rely on internal cluster metrics which aim to quantify the quality of the clusters. They are called internal because they're calculated from the clustered data itself rather than being compared to any ground truth. A common approach is to train multiple clustering models over a range of values of *k* and compare the cluster metrics to see which performs the best. Three most commonly used metrics are in my Google notes.

 * **centers** is our k.
 * **algorithm** is self-explanatory. What clustering algorithm do we want to use?
 
```{r}
kMeansParamSpace <-
    makeParamSet(makeDiscreteParam("centers", values = 3:8),
                 makeDiscreteParam("algorithm",
                                   values = c("Hartigan-Wong", "Lloyd", "MacQueen")))

gridSearch <- makeTuneControlGrid()

kFold <- makeResampleDesc("CV", iters = 10)

tunedK <- tuneParams(kMeans,
                     task = gvhdTask,
                     resampling = kFold,
                     par.set = kMeansParamSpace,
                     control = gridSearch,
                     measures = list(db, G1)) # Davies-Bouldin and pseduo F
```

Plot the results of hypertuning. Thankfully the algorithms largely agree as do the evaluation metrics. Computation time is also important to consider.

```{r}
kMeansTuningData <- generateHyperParsEffectData(tunedK)

longTuningData <- pivot_longer(kMeansTuningData$data,
                               c(-centers, -iteration, -algorithm),
                               names_to = "Metric",
                               values_to = "Value")

ggplot(longTuningData, aes(centers, Value, col = algorithm)) +
    facet_wrap(~ Metric, scales = "free_y") +
    geom_line() +
    geom_point() +
    theme_bw()
```

Train a k-means model. Always worth a visual inspection. You may need a different clustering algorithm like one that isn't spherical perhaps. It did an OK job at clustering, but it isn't perfect.

```{r}
tunedKMeans <- setHyperPars(kMeans, par.vals = tunedK$x)
tunedKMeansModel <- train(tunedKMeans, gvhdTask)
kMeansModelData <- getLearnerModel(tunedKMeansModel)

# Add cluster membership to our original data
gvhdTib <- gvhdTib %>% mutate(kMeansCluster = as.factor(kMeansModelData$cluster))

# Plot to see the clusters
ggpairs(gvhdTib, aes(col = kMeansCluster),
        upper = list(continuous = "density")) +
    theme_bw()
```

Predict new membership. They aren't really meant to predict, but it's useful for validating. We scale our new data using the existing scales from our training data.

```{r}
newCell <- tibble(CD4 = 510,
                  CD8b = 26,
                  CD3 = 500,
                  CD8 = 122) %>%
    scale(center = attr(gvhdScaled, "scaled:center"),
          scale = attr(gvhdScaled, "scaled:scale")) %>%
    as_tibble()

predict(tunedKMeansModel, newdata = newCell)
```

## Chapter 17

We will be using the hclust() function. It expects a distance matrix rather than raw data. A distance matrix contains the pairwise distances between each combination of elements. We can calculate the distances however we please (e.g. Euclidean, Manhattan).

We create a distance matrix using the dist() function.

Important to scale our data!

```{r}
gvhdDist <- dist(gvhdScaled, method = "euclidean")
gvhdHclust <- hclust(gvhdDist, method = "ward.D2")
```

Let's represent the data as a dendrogram. Remember the y-axis represents the distance between clusters based on whatever linkage and distance method was used. Since we used Ward's method the values of this axis are the within-cluster sum of squares. When clusters merge, they're connected by a horizontal line (which shows how similar the two clusters are). The ordering of cases along the x-axis is optimized so similar case are drawn near other to aid interpretation.

```{r}
gvhdDend <- as.dendrogram(gvhdHclust)
plot(gvhdDend, leaflab = "none") # turn off labels for each case
```

### How to choose the number of clusters

Or in other words, what level of the hierarchy should we use for clustering. To do this, we need to define a cut point. Cutting the tree near the top results in fewer clusters and cutting near the bottom results in more clusters. How can we choose? Our old friends Davies-Bouldin index, Dunn index, and the pseudo F-statistic can help.

With k-means clustering, we performed a cross-validation-like procedure for estimating the performance of different numbers of clusters. We cannot use this approach for hierarchical clustering because it cannot predict membership of new cases.

We will use bootstrapping. Take a bootstrap sample, apply some computation, and return a statistic. The mean of our statistic gives us the most likely value and the distribution gives us a sense of the stability of the statistic. Bootstrapping is randomly selecting cases with replacement to create a new sample the same size as the old sample.

We use bootstrapping to create multiple samples and generate a separate hierarchy for each. We select a range of cluster numbers from each hierarchy and calculate the internal cluster metrics for each. The bootstrapped samples allow us to calculate the stability of the internal metrics.

* data: the data we're passing
* clusters: Vector containing the cluster membership of every case in data.
* dist_matrix: Pass the pre-computed distance matrix for data.

Our function will return a list with the different performance indexes.

Strategy:

* Bootstrap sample from our data.
* Learn the hierarchy in each.
* Calculate performance indexes.
* Find the mean and variance.

```{r}
# create our function to calculate the performance indexes
cluster_metrics <- function(data, clusters, dist_matrix) {
    list(db = index.DB(data, clusters)$DB, # the statistic is contained with $DB
         G1 = index.G1(data, clusters),
         dunn = dunn(dist_matrix, clusters),
         clusters = length(unique(clusters)))
}

# create our bootstrap samples
gvhdBoot <- map(1:10, ~ {
    gvhdScaled %>%
        as_tibble() %>%
        sample_n(size = nrow(.), replace = T)
})

metricsTib <- map_df(gvhdBoot, function(boot) {
    
    # compute Euclidean distance matrix
    d <- dist(boot, method = "euclidean")
    
    # Compute cluster hierarchy
    cl <- hclust(d, method = "ward.D2")
    
    # select 3 to 8 clusters as our cut points
    map_df(3:8, function(k) {
        
        # vector containing cluster membership of every case
        cut <- cutree(cl, k = k)
        
        # calculate the performance metrics for each cluster
        cluster_metrics(boot, clusters = cut, dist_matrix = d)
    })
})
```

### Plot our results

Seems like best number of clusters is 4.

```{r}
metricsTib <-
    metricsTib %>%
    mutate(bootstrap = factor(rep(1:10, each = 6))) %>%
    pivot_longer(c(-clusters, -bootstrap),
                 names_to = "Metric",
                 values_to = "Value")

ggplot(metricsTib, aes(as.factor(clusters), Value)) +
    facet_wrap(~ Metric, scales = "free_y") +
    geom_line(size = 0.1, aes(group = bootstrap)) +
    
    # connect the mean across all bootstrap samples
    geom_line(stat = "summary", fun.y = "mean", aes(group = 1)) +
    
    # used to visualize the 95% CIs for boostrap samples
    # not mean_cl_normal which assumes normal distribution (may or may not be true)
    # the crossbar argument draws a sort of box and whisker plot
    stat_summary(fun.data="mean_cl_boot",
                 geom="crossbar", width = 0.5, fill = "white") +
    theme_bw()
```

### Cutting the trees

```{r}
gvhdCut <- cutree(gvhdHclust, k = 4)
plot(gvhdDend, leaflab = "none")
rect.hclust(gvhdHclust, k = 4)
```

### Graphing the clusters

```{r}
gvhdTib <- mutate(gvhdTib, hclustCluster = as.factor(gvhdCut))

ggpairs(gvhdTib, aes(col = hclustCluster),
        upper = list(continuous = "density"),
        lower = list(continuous = wrap("points", size = 0.5))) +
    theme_bw()
```

### How stable are our clusters?

We can quantify how well the cluster memberships agree with each other between bootstrap samples. This agreement is called cluster stability. A common way to quantify this stability is to use the Jaccard index.

It quantifies the similarity between two sets of discrete variables. Its value can be interpreted as the percentage of total values that are present in both sets. It ranges from 0% (no common values) to 100% (all values common in both sets).

j = Number of values in both sets / Total number of unique values * 100.

```{r}
a <- c(3,3,5,2,8)
b <- c(1,3,5,6)

# 6 unique values
# 2 values in common
# Jaccard index
2 / 6
```

If we cluster on multiple bootstrap samples, we can calculate the Jaccard index between the "original" clusters (the clusters on all the data) an each of the bootstrap samples and take the mean. If the index is low, then cluster membership is changing considerably between bootstrap samples indicating our clustering result is unstable and may not generalize well. If the index is high, then cluster membership is changing very little indicating stable clusters.

* First argument is our distance matrix. It could also handle raw data.
* B is the number of bootstrap samples we wish to calculate.
* clustermethod is which type of clustering model we wish to build. For hierarchical clustering we use disthclustCBI.
* The k argument specifies the number of clusters.
* method specifies the distance method to use for clustering.
* showplots tells the function whether or not to return the plots too.

```{r}
par(mfrow = c(3, 4))

clustBoot <- clusterboot(gvhdDist,
                         B = 10,
                         clustermethod = disthclustCBI,
                         k = 4,
                         cut = "number",
                         method = "ward.D2",
                         showplots = T)

# returns the clusterwise Jaccard bootstrap means. They're high suggesting stability!
clustBoot

# The first (top-left) and last (bottom-right) plots show clustering on original, full dataset.
# Each plot in-between shows a different bootstrap sample. Good way to visualize
# the stability.
```

