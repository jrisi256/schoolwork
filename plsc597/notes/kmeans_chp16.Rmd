---
title: "Chp. 16 - Kmeans"
output: html_document
---

```{r}
library(mlr)
library(clue)
library(mclust)
library(GGally)
library(tidyverse)
library(clusterSim)
```

```{r}
data(GvHD, package = "mclust")
gvhdTib <- as_tibble(GvHD.control)
gvhdScaled <- gvhdTib %>% scale()
```

We use the upper, lower, and diag arguments to specify what kind of plots we want to be drawn above, below, and on the diagonal. Each argument takes a list where each list specifies a type of plot. 2D Density plots above the diagonal, scatterplots below, and density plots on the diagonal. You can wrap the plot type and include graphical options within the wrap.

```{r}
ggpairs(GvHD.control,
        upper = list(continuous = "density"),
        lower = list(continuous = wrap("points", size = 0.5)),
        diag = list(continuous = "densityDiag")) +
    theme_bw()
```

* The *par.vals* argument is for passing hyperparameters.
* *iter.max* sets an upper limit for the number of times the algorithm will cycle through the data.
* *nstart* is how many times the algorithm will randomly initialize the centers. For each set of initial centers, the cases are assigned to whatever cluster they're closest to like normal. The set with the smallest within-cluster sum of squared error is then the set used for the rest of the algorithm. Arguably more important than *iter.max* as it creates centroids more similar to the real centroids.

```{r}
gvhdTask <- makeClusterTask(data = as.data.frame(gvhdScaled))
kMeans <- makeLearner("cluster.kmeans", par.vals = list(iter.max = 100,
                                                        nstart = 10))
```

Choosing the number of clusters is hard. If you have some theoretical knowledge about what the number should be, then use that. If you're using it in a preprocessing step then you can include it as a hyperparameter in the model-building process, and pick whatever improves model performance the most.

People have proposed many different methods for measuring cluster over and underfitting. They rely on internal cluster metrics which aim to quantify the quality of the clusters. They are called internal because they're calculated from the clustered data itself rather than being compared to any ground truth. A common approach is to train multiple clustering models over a range of values of *k* and compare the cluster metrics to see which performs the best. Three most commonly used metrics are in my Google notes.

 * **centers** is our k.
 * **algorithm** is self-explanatory. What clustering algorithm do we want to use?
 
```{r}
kMeansParamSpace <-
    makeParamSet(makeDiscreteParam("centers", values = 3:8),
                 makeDiscreteParam("algorithm",
                                   values = c("Hartigan-Wong", "Lloyd", "MacQueen")))

gridSearch <- makeTuneControlGrid()

kFold <- makeResampleDesc("CV", iters = 10)

tunedK <- tuneParams(kMeans,
                     task = gvhdTask,
                     resampling = kFold,
                     par.set = kMeansParamSpace,
                     control = gridSearch,
                     measures = list(db, G1)) # Davies-Bouldin and pseduo F
```

Plot the results of hypertuning. Thankfully the algorithms largely agree as do the evaluation metrics. Computation time is also important to consider.

```{r}
kMeansTuningData <- generateHyperParsEffectData(tunedK)

longTuningData <- pivot_longer(kMeansTuningData$data,
                               c(-centers, -iteration, -algorithm),
                               names_to = "Metric",
                               values_to = "Value")

ggplot(longTuningData, aes(centers, Value, col = algorithm)) +
    facet_wrap(~ Metric, scales = "free_y") +
    geom_line() +
    geom_point() +
    theme_bw()
```

Train a k-means model. Always worth a visual inspection. You may need a different clustering algorithm like one that isn't spherical perhaps. It did an OK job at clustering, but it isn't perfect.

```{r}
tunedKMeans <- setHyperPars(kMeans, par.vals = tunedK$x)
tunedKMeansModel <- train(tunedKMeans, gvhdTask)
kMeansModelData <- getLearnerModel(tunedKMeansModel)

# Add cluster membership to our original data
gvhdTib <- gvhdTib %>% mutate(kMeansCluster = as.factor(kMeansModelData$cluster))

# Plot to see the clusters
ggpairs(gvhdTib, aes(col = kMeansCluster),
        upper = list(continuous = "density")) +
    theme_bw()
```

Predict new membership. They aren't really meant to predict, but it's useful for validating. We scale our new data using the existing scales from our training data.

```{r}
newCell <- tibble(CD4 = 510,
                  CD8b = 26,
                  CD3 = 500,
                  CD8 = 122) %>%
    scale(center = attr(gvhdScaled, "scaled:center"),
          scale = attr(gvhdScaled, "scaled:scale")) %>%
    as_tibble()

predict(tunedKMeansModel, newdata = newCell)
```
