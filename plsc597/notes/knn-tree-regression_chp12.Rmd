---
title: "kNN and tree-based regression, Chp. 12"
output: html_document
---

```{r}
library(mlr)
library(kknn)
library(parallel)
library(tidyverse)
library(parallelMap)
```

* The **heatan** variable is the amount of energy released by a certain quantity of fuel when it is combusted.
* The **h20** variable is the percentage of humidity in the fuel's container.
* Remaining variables show how much ultraviolet or near-infrared light of a particular wavelength each batch of fuel absorbs (each variable represents a different length).

```{r}
data("fuelsubset.task")
fuel <- getTaskData(fuelsubset.task)
fuelTib <- as_tibble(fuel)
```

Let's plot some graphs to see some relationships. It's very pretty! And highly nonlinear. This data is known as **spectral data** because it contains observations made across a range of wavelengths. We're measuring how much a substance absorbs light from a range of different colors.

This is known as **functional data** because there are many dimensions in the dataset (the wavelengths measured across), and there is an order to those dimensions (starting by measuring the absorbance at the lowest wavelength and working our way to the highest wavelength).

It's more useful to think of each item of the sample as being observed on a continuum. Another example might be thinking about the growth curves of many children: the unit of observation is the growth curve of one child.

```{r}
fuelUntidy <-
    fuelTib %>%
    mutate(id = row_number()) %>%
    pivot_longer(c(-heatan, -h20, -id),
                 names_to = "variable",
                 values_to = "absorbance") %>%
    mutate(spectrum = str_sub(variable, 1, 3),
           wavelength = as.numeric(str_extract(variable, "(\\d)+")))

fuelUntidy %>%
    ggplot(aes(absorbance, heatan, col = as.factor(wavelength))) +
    facet_wrap(~ spectrum, scales = "free_x") +
    geom_smooth(se = FALSE, size = 0.2) +
    ggtitle("Absorbance vs heatan for each wavelength") +
    theme_bw() +
    theme(legend.position = "none")

fuelUntidy %>%
    ggplot(aes(wavelength, absorbance, group = id, col = heatan)) +
    facet_wrap(~ spectrum, scales = "free_x") +
    geom_smooth(se = FALSE, size = 0.2) +
    ggtitle("Wavelength vs absorbance for each batch") +
    theme_bw()

fuelUntidy %>%
    ggplot(aes(h20, heatan)) +
    geom_smooth(se = FALSE) +
    ggtitle("Humidity vs heatan") +
    theme_bw()
```

## Building our kNN regression model

```{r}
fuelTask <- makeRegrTask(data = fuelTib, target = "heatan")
kknn <- makeLearner("regr.kknn")
```

Tune the k hyperparameter. Grid-search meaning we will search through every value. Then we define a 10-fold cross validation strategy.

```{r}
kknnParamSpace <- makeParamSet(makeDiscreteParam("k", values = 1:12))
gridSearch <- makeTuneControlGrid()
kFold <- makeResampleDesc("CV", iters = 10)
tunedK <- tuneParams(kknn,
                     task = fuelTask,
                     resampling = kFold,
                     par.set = kknnParamSpace,
                     control = gridSearch)
```

Plot the hyperparameter tuning process to get a sense if we need to expand our search.

```{r}
knnTuningData <- generateHyperParsEffectData(tunedK)
plotHyperParsEffect(knnTuningData,
                    x = "k",
                    y = "mse.test.mean",
                    plot.type = "line")
```

Train the kNN model using our hyperparameter values.

```{r}
tunedKnn <- setHyperPars(kknn, par.vals = tunedK$x)
tunedKnnModel <- train(tunedKnn, fuelTask)
```

## Build our Random Forest Regression Model

```{r}
forest <- makeLearner("regr.randomForest")
```

Let's do some hyperparameter tuning. We pick 100 different combinations of our hyperparameter values, and we use 10-fold cross-validation on each of those 100 different combinations.

```{r}
forestParamSpace <-
    makeParamSet(makeIntegerParam("ntree", lower = 50, upper = 50),
                 makeIntegerParam("mtry", lower = 100, upper = 367),
                 makeIntegerParam("nodesize", lower = 1, upper = 10),
                 makeIntegerParam("maxnodes", lower = 5, upper = 30))

randSearch <- makeTuneControlRandom(maxit = 100)

parallelStartSocket(cups = detectCores())

tunedForestPars <- tuneParams(forest,
                              task = fuelTask,
                              resampling = kFold,
                              par.set = forestParamSpace,
                              control = randSearch)

parallelStop()
```

Train our random forest and plot the out-of-bar error to see if we've included enough trees. It seems to sort of stabilize, but it could also keep going down a bit more if we included more trees. Hard to say. This is purely for demonstration purposes so we move on.

```{r}
tunedForest <- setHyperPars(forest, par.vals = tunedForestPars$x)
tunedForestModel <- train(tunedForest, fuelTask)

forestModelData <- tunedForestModel$learner.model
plot(forestModelData)
```

## Build our XGBoost model

```{r}
xgb <- makeLearner("regr.xgboost")
```

Tune our hyperparameters. Same approach we took for tuning the random forest.

```{r}
xgbParamSpace <-
    makeParamSet(makeNumericParam("eta", lower = 0, upper = 1),
                 makeNumericParam("gamma", lower = 0, upper = 10),
                 makeIntegerParam("max_depth", lower = 1, upper = 20),
                 makeNumericParam("min_child_weight", lower = 1, upper = 10),
                 makeNumericParam("subsample", lower = 0.5, upper = 1),
                 makeNumericParam("colsample_bytree", lower = 0.5, upper = 1),
                 makeIntegerParam("nrounds", lower = 30, upper = 30))

tunedXgbPars <- tuneParams(xgb,
                           task = fuelTask,
                           resampling = kFold,
                           par.set = xgbParamSpace,
                           control = randSearch)
```

Train the model and plot the Root Mean Square Error to see if we included enough trees in our ensemble. Things appears to have stabilized at 30 trees so we should be good.

```{r}
tunedXgb <- setHyperPars(xgb, par.vals = tunedXgbPars$x)
tunedXgbModel <- train(tunedXgb, fuelTask)
xgbModelData <- tunedXgbModel$learner.model

ggplot(xgbModelData$evaluation_log, aes(iter, train_rmse)) +
    geom_line() +
    geom_point() +
    theme_bw()
```

Let's benchmark our models to see which one performs the best. We're going to use simple holdout cross-validation this time around.

```{r}
kknnWrapper <- makeTuneWrapper(kknn,
                               resampling = kFold,
                               par.set = kknnParamSpace,
                               control = gridSearch)

forestWrapper <- makeTuneWrapper(forest,
                                 resampling = kFold,
                                 par.set = forestParamSpace,
                                 control = randSearch)

xgbWrapper <- makeTuneWrapper(xgb,
                              resampling = kFold,
                              par.set = xgbParamSpace,
                              control = randSearch)

learners <- list(kknnWrapper, forestWrapper, xgbWrapper)

holdout <- makeResampleDesc("Holdout")
bench <- benchmark(learners, fuelTask, holdout)
```
